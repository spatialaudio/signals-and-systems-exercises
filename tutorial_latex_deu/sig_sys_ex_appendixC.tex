\clearpage
\section{Appendix C: DFT in der Welt der linearen Algebra}
{\tiny 39AF81A22A}
%
\noindent Die DFT ist keine Erfindung der SigSys, sondern hat eine
tiefsinnige Verknüpfung zu Linearkombinationen einer orthogonalen
Vektorbasis.
%
Wenn wir also die $N$ Samples der Signalfolge $x[k]$ und die $N$ Koeffizienten des Spektrums $X[\mu]$
als $N$-dimensionale Vektoren auffassen, können wir die Werkzeuge der linearen
Algebra benutzen und die DFT als Eigenwertproblem interpretieren.
%
Das Erlernen dieser Sichtweise ist sehr empfehlenswert,
weil lineare Algebra als wichtiges Grundwerkzeug für heutige
Big Data und Machine Learning Probleme gilt, vgl. \cite{Strang2019}.
%
Die DFT und die Singulärwertzerlegung (englisch
Singular Value Decomposition (SVD)) sind im Zeitalter der Data Science
fundamentale Werkzeuge, die wir im Schlaf beherrschen sollten!
%

Da nun $x[k]$ und $X[\mu]$ periodisch in ihrer Länge $N$ aufgefasst werden,
haben wir es mit zyklischen Vektoren und Matrizen zu tun.
%
Eine spezielle zyklische Matrix ist die Permutationsmatrix, das wird unser
Ausgangspunkt für die folgende Betrachtung, welche stark inspiriert von
den sehr empfehlenswerten Büchern \cite{Strang2016, Strang2019} ist.
%

Starten wir mit einer Permutationsmatrix $\bm{P}$ mit Dimension $N \times N$,
beispielhaft für $N=4$
\begin{align}
\bm{P}_{N=4} =
\begin{bmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
\end{bmatrix}.
\end{align}
%
\textbf{Eigenwerte von $\bm{P}$}:
Es gibt $N$ Eigenwerte $\lambda_{0}, \lambda_{1}, \lambda_{2}, \dots, \lambda_{N-1}$,
folgend aus der Bedingung
\begin{align}
\mathrm{det}(\bm{P}-\lambda \bm{I}) = 0 \rightarrow \lambda^N - 1 = 0 \rightarrow \lambda^N = 1
\end{align}
%
Mit der komplexen Zahl mit Betrag 1
\begin{align}
W_N = \e^{+\im\frac{2\pi}{N}}
\end{align}
sind diese $N$ Eigenwerte für $0\leq n \leq N-1$ gegeben als
\begin{align}
\lambda_n =(W_N)^n = W_N^n = \e^{+\im\frac{2\pi}{N}\cdot n}.
\end{align}
Das sind komplexe Zahlen mit Betrag 1, also auf dem Einheitskreis in der komplexen
Ebene liegend und äquiangular verteilt alle $\frac{2\pi}{N}$.
%
Für z.B. $N=4$ bekommen wir (äquiangular alle 90 Grad)
\begin{align}
\lambda_0 =(W_4)^0 = W_4^0 = \e^{+\im\frac{2\pi\cdot 0}{4}} = +1\nonumber\\
\lambda_1 =(W_4)^1 = W_4^1 = \e^{+\im\frac{2\pi\cdot 1}{4}} = +\im\nonumber\\
\lambda_2 =(W_4)^2 = W_4^2 = \e^{+\im\frac{2\pi\cdot 2}{4}} = -1\nonumber\\
\lambda_3 =(W_4)^3 = W_4^3 = \e^{+\im\frac{2\pi\cdot 3}{4}} = -\im,
\end{align}
skizziert in der Grafik unten. Wir sortieren die Eigenwerte hier mit
zunehmender Winkelgröße. Wenn wir das numerisch mit einem Computer auswerten,
können die Eigenwerte und zugehörige Eigenvektoren auch anders sortiert bzw.
rotiert/invertiert sein,
da also besonders drauf schauen!
\begin{center}
\begin{tikzpicture}[scale=1.5]
\def \tic {0.05}
%
% basic diagram features:
%
\draw[C7, thick] (0,0) circle(1);  % unit circle, i.e. DTFT domain
%
\draw (1+2*\tic,-3*\tic) node{$1$}; % indicate that this is the unit circle
\draw[->] (-1.25,0)--(1.5,0) node[right]{$\Re\{\lambda\}$}; % axis label
\draw[->] (0,-1.25)--(0,1.5) node[above]{$\Im\{\lambda\}$}; % axis label
%
\draw[C0, ultra thick] (0,+1) node{\Huge $\circ$};
\draw[C0, ultra thick] (0,-1) node{\Huge $\circ$};
\draw[C0, ultra thick] (+1,0) node{\Huge $\circ$};
\draw[C0, ultra thick] (-1,0) node{\Huge $\circ$};
%
\draw[] (+1+4*\tic,4*\tic) node{$\lambda_0$};
\draw[] (4*\tic,+1+4*\tic) node{$\lambda_1$};
\draw[] (-1+4*\tic,4*\tic) node{$\lambda_2$};
\draw[] (4*\tic,-1+4*\tic) node{$\lambda_3$};
%
\draw[] (-1.75,1) node{Lösungen für $\lambda^4 = 1$};
\end{tikzpicture}
\end{center}


\textbf{Eigenvektoren von $\bm{P}$}:
Zu diesen $N$ individuellen Eigenwerten gehören $N$ Eigenvektoren. Eine orthogonale
Matrix hat orthogonale Eigenvektoren. Weil wir per Sicht sehen und auch wissen,
dass $\bm{P}$ orthogonal (hier sogar orthonormal) ist,
können wir also $N$ orthogonale
Eigenvektoren erwarten.
Für den $n$-ten Eigenvektor $\bm{x}_n$ gilt mit $0\leq n \leq N-1$
\begin{align}
\bm{P} \bm{x}_n = \lambda_n \bm{x}_n.
\end{align}
Der $n$-te Eigenvektor $\bm{x}_n$ definiert sich zu
\begin{align}
\bm{x}_n =
\begin{bmatrix}
(\lambda_n)^0\\
(\lambda_n)^1\\
(\lambda_n)^2\\
(\lambda_n)^3\\
\vdots\\
(\lambda_n)^{N-1}\\
\end{bmatrix}
\end{align}
%
Für z.B. $N=4$ bekommen wir die 4 (zum Teil komplexwertigen) Eigenvektoren
\begin{align}
\bm{x}_0 =
\begin{bmatrix}
(W_4^0)^0\\
(W_4^0)^1\\
(W_4^0)^2\\
(W_4^0)^3
\end{bmatrix}
=
\begin{bmatrix}
1\\1\\1\\1
\end{bmatrix}
\quad
\bm{x}_1 =
\begin{bmatrix}
(W_4^1)^0\\
(W_4^1)^1\\
(W_4^1)^2\\
(W_4^1)^3
\end{bmatrix}
=
\begin{bmatrix}
1\\\im\\-1\\-\im
\end{bmatrix}\quad
\bm{x}_2 =
\begin{bmatrix}
(W_4^2)^0\\
(W_4^2)^1\\
(W_4^2)^2\\
(W_4^2)^3
\end{bmatrix}
=
\begin{bmatrix}
1\\-1\\1\\-1
\end{bmatrix}\quad
\bm{x}_3 =
\begin{bmatrix}
(W_4^3)^0\\
(W_4^3)^1\\
(W_4^3)^2\\
(W_4^3)^3
\end{bmatrix}
=
\begin{bmatrix}
1\\-\im\\-1\\\im
\end{bmatrix}
\end{align}
%
%Wir sehen, dass $\bm{x}_1 = \bm{x}_3^*$, das werden wir wiedersehen.

\textbf{Fourier Matrix, i.e. Eigenvektoren von $\bm{P}$}:
Wir setzen eine orthogonale Einheitsvektormatrix
auf, indem wir die Vektoren mit
wachsendem Winkel in $W_N^n$ sortieren, zunächst mit $\lambda_n$ geschrieben
\begin{align}
\bm{F}_N =
\begin{bmatrix}
\bm{x}_0 \quad \bm{x}_1 \quad \bm{x}_2 \quad \bm{x}_3 \quad \dots \quad \bm{x}_{N-1}
\end{bmatrix}
=
\begin{bmatrix}
\lambda^0_0 & \lambda^0_1 & \lambda^0_2 & \lambda^0_3 & \dots & \lambda^0_{N-1}\\[1em]
\lambda^1_0 & \lambda^1_1 & \lambda^1_2 & \lambda^1_3 & \dots & \lambda^1_{N-1}\\[1em]
\lambda^2_0 & \lambda^2_1 & \lambda^2_2 & \lambda^2_3 & \dots & \lambda^2_{N-1}\\[1em]
\lambda^3_0 & \lambda^3_1 & \lambda^3_2 & \lambda^3_3 & \dots & \lambda^3_{N-1}\\[1em]
\vdots & \vdots & \vdots &\vdots &\ddots & \vdots\\[1em]
\lambda^{N-1}_0 & \lambda^{N-1}_1 & \lambda^{N-1}_2 & \lambda^{N-1}_3 & \dots & \lambda^{N-1}_{N-1} & \\
\end{bmatrix}
\end{align}
und die sogenannte Fourier Matrix mit $W_N^n$ geschrieben (in dieser Form oft in Büchern)
\begin{align}
\label{eq:DFT_FMatrix_WN}
\bm{F}_N =
\begin{bmatrix}
\bm{x}_0 \quad \bm{x}_1 \quad \bm{x}_2 \quad \bm{x}_3 \quad \dots \quad \bm{x}_{N-1}
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 1 & 1 & \dots & 1\\[1em]
1 & W_N^1 & W_N^2 & W_N^3 & \dots & W_N^{(N-1)}\\[1em]
1 & W_N^2 & W_N^4 & W_N^6 & \dots & W_N^{2(N-1)}\\[1em]
1 & W_N^3 & W_N^6 & W_N^9 & \dots & W_N^{3(N-1)}\\[1em]
\vdots & \vdots & \vdots &\vdots &\ddots & \vdots\\[1em]
1 & W_N^{(N-1)} & W_N^{2(N-1)} & W_N^{3(N-1)} & \dots & W_N^{(N-1)(N-1)}
\end{bmatrix}
\end{align}
Machen wir uns klar, dass a) alle $\lambda_0^{(\cdot)} = 1$, weil $\lambda_0=1$
und b) alle $\lambda_{(\cdot)}^0 = 1$, weil $\mathbb{C}^0 = 1$.
%

\noindent Für z.B. $N=4$ bekommen wir die Fouriermatrix
\begin{align}
\bm{F}_4 =
\begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & \im & -1 & -\im \\
1 & -1 & 1 & -1 \\
1 & -\im & -1 & \im
\end{bmatrix}.
\end{align}

Mit dem äußeren Vektorprodukt können wir die Index-Matrix
\begin{align}
\bm{K}_N =
\begin{bmatrix}
0\\1\\2\\3\\\vdots\\N-1
\end{bmatrix}
[0,1,2,3,\dots,N-1]
=
\begin{bmatrix}
0 & 0 & 0 & 0 & \dots & 0\\
0 & 1 & 2 & 3 & \dots & N-1\\
0 & 2 & 4 & 6 & \dots & 2(N-1)\\
0 & 3 & 6 & 9 & \dots & 3(N-1)\\
: & : & : & : & \ddots & :\\
0 & (N-1) & 2(N-1) & 3(N-1) & \dots & (N-1)(N-1)\\
\end{bmatrix}
\end{align}
erzeugen und aus dieser die symmetrische Fourier Matrix
\begin{align}
\bm{F} = \exp(+\im\frac{2\pi}{N} \bm{K}) =
\e^{+\im\frac{2\pi}{N} \odot \bm{K}} =
(W_N)^{\bm{K}}.
\end{align}
Dies lässt sich in Source Code sehr effizient benutzen. In Matlab und Python
ginge das jeweils mit drei Befehlen (wenn wir vorher $N$ und
für Python \texttt{import numpy as np} definiert haben)

\verb|k = [0:N-1].'; K = k * k.'; F = exp(+1j*2*pi/N * K);|

\verb|k = np.arange(N); K = np.outer(k, k); F = np.exp(+1j*2*np.pi/N * K)|

\noindent Die Matrix $\bm{F}_N$ hat, weil wir sie intentional so zusammengebaut haben,
vollen Rang $N$. Sie spannt also einen $N$-dimensionalen, komplexwertigen Vektorraum auf,
der mit der Linearkombination der $N$ orthogonalen Eigenvektoren $\bm{x}$
dargestellt wird. Es ist also eine $N$-dimensionale, orthogonale Vektorbasis,
sozusagen das 'Beste' was wir in eine Matrix packen können.
%
Noch besser ist nur noch die Matrix $\mathring{\bm{F}} = \bm{F}/\sqrt{N}$, weil sie sogar
ortho\textbf{normal} ist.
Sie ist zudem eine unitäre Matrix, und weil immer noch symmetrisch, gelten:
\begin{align}
\mathring{\bm{F}} = \mathring{\bm{F}}^\mathrm{T}
\qquad
\mathring{\bm{F}}^* =
\mathring{\bm{F}}^\mathrm{H} =
\mathring{\bm{F}}^{-1}
\qquad\text{und}\qquad
\mathring{\bm{F}}^{-1} \, \mathring{\bm{F}} =
\mathring{\bm{F}} \, \mathring{\bm{F}}^{-1} =
\mathbf{I}.
\end{align}

\textbf{Diagonalisierung von $\bm{P}$:}
Passend zu der Eigenvektormatrix (Fourier Matrix)
$\bm{F}_N$ definieren wir die Eigenwerte-Diagonalmatrix
\begin{align}
\bm{\Lambda}_N =
\begin{bmatrix}
\lambda_0 & & & & &\\
&\lambda_1 & & & & \\
&&\lambda_2 & & & \\
&&&\lambda_3 & & \\
&&&& \ddots& & \\
&&&&&\lambda_{N-1}
\end{bmatrix}
\end{align}
Mit diesen beiden Matrizen, also $\bm{F}$ und $\bm{\Lambda}$ wird nun die
Permutationsmatrix diagonalisiert
\begin{align}
\bm{P}  = \bm{F} \bm{\Lambda} \bm{F}^{-1} \quad\text{bzw.}\quad
\bm{F}^{-1} \bm{P} \bm{F} = \bm{\Lambda}
\end{align}
Eine wichtige Eigenschaft von Matrizen mit orthogonalen, komplexen Vektoren
ist der Zusammenhang zwischen Inverser und konjugiert-komplexer Matrix.
Es gilt ja zunächst
\begin{align}
\bm{F}\bm{F}^* = N \bm{I}, \qquad \bm{F}^*\bm{F} = N \bm{I}
\end{align}
woraus
\begin{align}
%\bm{F}^{-1}\bm{F}\bm{F}^* = N \bm{F}^{-1} \bm{I}\\
%\frac{1}{N}\bm{F}^* = \bm{F}^{-1}\\
\bm{F}^{-1} = \frac{1}{N} \bm{F}^*
\end{align}
folgt.
Wenn wir nun noch zur Kenntnis nehmen, dass $\bm{F}$ eine symmetrische Matrix ist,
also $\bm{F}=\bm{F}^\mathrm{T}$, können wir den $^\mathrm{H}$-Operator (für die adjungierte
Matrix), also konjugiert-komplex und transponiert benutzen
\begin{align}
\bm{F}\bm{F}^\mathrm{H} = N \bm{I},\qquad
\bm{F}^\mathrm{H}\bm{F} = N \bm{I},\qquad
\bm{F}^{-1} = \frac{1}{N} \bm{F}^\mathrm{H}.
\end{align}
Die letzte Eigenschaft ist sehr wichtig, weil wir die Inverse durch eine
gewichtete Adjungierte bzw. noch besser als konjugiert-komplexe (dann muss der Rechner
nicht Transponieren) darstellen können.
%
Die obige Diagonalisierung können wir also auch
\begin{align}
\bm{P}  = \bm{F} \bm{\Lambda} \frac{1}{N} \bm{F}^\mathrm{H} \quad\text{bzw.}\quad
\frac{1}{N} \bm{F}^\mathrm{H} \bm{P} \bm{F} = \bm{\Lambda}
\end{align}
schreiben.

\textbf{DFT in Matrix-Notation}
Warum machen wir das alles? Um vorzugreifen:
%
Wenn wir ein $N$ Werte DFT-Spektrum $X[\mu]$ als Spaltenvektor $\bm{x}_\mu$ darstellen
\begin{align}
\bm{x}_\mu = [X[\mu=0],X[1],X[2],X[3],\dots,X[N-1]]^\mathrm{T}
\end{align}
also die DFT-Werte für Index $0 \leq \mu \leq N-1$ nehmen
(die DFT ist zwar $N$-periodisch,
aber in einer einzigen Periode steckt ja die gesamte Information drin),
dann bekommen wir mit
\begin{align}
\bm{x}_k = \frac{1}{N} \bm{F} \bm{x}_\mu
\end{align}
einen Spaltenvektor
\begin{align}
\bm{x}_k = [x[k=0],x[1], x[2], x[3], \dots, x[N-1]]^\mathrm{T}
\end{align}
mit dem Signal im Zeitbereich für $0 \leq k \leq N-1$.
Dies ist die inverse DFT als Matrix-Operation notiert, und offensichtlich
benutzen wir die oben definierte $\bm{F}$-Matrix!
%
Die Hintransformation der DFT
(also vom Zeitbereich in den Bildbereich) lautet in Matrix-Notation
\begin{align}
\bm{x}_\mu = \bm{F}^\mathrm{H} \bm{x}_k
\end{align}
%
Dass dieses Transformationspaar in sich geschlossen ist, also
$x[k] = \text{IDFT}(\text{DFT}(x[k]))$,
lässt sich mit den Matrizen und deren Eigenschaften mühelos checken
\begin{align}
\bm{x}_k =
\frac{1}{N} \bm{F} (\bm{F}^\mathrm{H} \bm{x}_k)=
\frac{1}{N} \underbrace{\bm{F} \bm{F}^\mathrm{H}}_{N \bm{I}} \bm{x}_k = \bm{x}_k.
\end{align}
In DFT/IDFT-Summendarstellungen müssten wir für diesen 'Beweis' deutlich mehr Aufwand
betreiben.

Was wir jetzt verstehen müssen, warum diese beiden Matrix-Operationen
genau der inversen DFT und der DFT entsprechen, also hinter den Zahlen
und der Matrix-Multiplikation das Wesen der angelegten Matrizen kennenzulernen.
%
Gilbert Strang hat das---wie alles, was er didaktisch auf den Prüfstand
stellt---herausragend in seinen Büchern
\cite{Strang2010, Strang2016, Strang2019}
aufgeschrieben; (Alters)-Weisheit und progressiver Blick in die konstruktive
Zukunft bestens vereint. Algebra löst viel
mehr heutiger Ingenieursprobleme als Analysis, und wenn wir in der Lage sind,
die Algorithmen des maschinellen Lernens und die 'klassische' SigSys in der Welt
der linearen Algebra zu verorten, sind wir für zukünftige Ingenieursberufsprofile
bestens vorbereitet. Genau dieses didaktische Konzept verfolgt Strang seit vielen
Jahren mit Erfolg am MIT, daher sehr zu empfehlende Lektüre.

\textbf{Zyklische Permutationen}: Wir könnten uns nun fragen, welche
Matrizen auch mit $\bm{F}$ diagonalisiert werden können, weil das ja mutmaßlich
eine ganz sinnstiftende Matrixeigenschaft ist.
%
Weil wir bei der DFT mit Periodizität zu tun haben, $x[k]$ und $X[\mu]$
sind ja jeweils $N$-periodische Folgen, sollten wir uns zunächst
mal Permutationen von
der Permutationsmatrix anschauen, also z.B. für $N=4$

\begin{align}
\bm{P}^2 = \bm{P}\bm{P} =
\begin{bmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
\end{bmatrix}.
\end{align}
%
Die dritte Zeile von Matrix zwei ist verantwortlich für die dritte Zeile der Ergebnis-Matrix: wir berücksichtigen wegen [0,0,0,1] nur die vierte Zeile von Matrix eins.
Das ist Matrixmultiplikation als Permutation gedacht.

\begin{align}
\bm{P}^3 = \bm{P}\bm{P}\bm{P} =
\begin{bmatrix}
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
\end{bmatrix}\qquad
\bm{I} = \bm{P}^4 = \bm{P}\bm{P}\bm{P}\bm{P} =
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{bmatrix}
\end{align}
Wir können das Spiel unendlich fortsetzen, aber bei $\bm{I} = \bm{P}^N$ haben
wir alle möglichen unterschiedlichen Permutationsmuster durch. Mehr Variation
gibt die $N$-Basis nicht her, wir sind zyklisch in $N$.

Jetzt wird es fancy: alle permutierten Permutationsmatrizen haben a)
die gleichen Eigenvektoren $\bm{x}_n$ (und damit die gleiche Fouriermatrix $\bm{F}$,
das war ja unsere Ausgangsfrage) und b) Eigenwerte die auf $\lambda_n$ basieren.
Es gilt nämlich
\begin{align}
&\bm{P}^1 \bm{x}_n = \lambda_n^1 \bm{x}_n
\qquad
\bm{P}^1 \bm{x}_n = (W_N^n)^{1} \cdot \bm{x}_n
\nonumber\\
&\bm{P}^2 \bm{x}_n = \lambda_n^2 \bm{x}_n
\qquad
\bm{P}^2 \bm{x}_n = (W_N^n)^{2} \cdot \bm{x}_n = W_N^{2 n} \bm{x}_n
\nonumber\\
&\bm{P}^3 \bm{x}_n = \lambda_n^3 \bm{x}_n
\qquad
\bm{P}^3 \bm{x}_n = (W_N^n)^{3} \cdot \bm{x}_n = W_N^{3 n} \bm{x}_n
\nonumber\\
&\vdots\nonumber\\
&\bm{P}^N \bm{x}_n = \lambda_n^N \bm{x}_n
\qquad
\bm{P}^N \bm{x}_n = (W_N^n)^{N} \cdot \bm{x}_n = W_N^{N\,n} \bm{x}_n
\end{align}
%
Anhand $\lambda_n^N$ stellen wir kurz sicher, dass wir mit der Formelschreibweise
vertraut sind.
%
Wir hatten oben für $0\leq n \leq N-1$ die $N$ Eigenwerte von $\bm{P}$
eingeführt als
\begin{align}
\lambda_n =(W_N)^n = W_N^n = \e^{+\im\frac{2\pi\cdot n}{N}}.
\end{align}
Die Eigenwerte von $\bm{I} = \bm{P}^N$ sind nun
\begin{align}
\lambda_n^N =\left((W_N)^n\right)^N = W_N^{(n \cdot N)} = \e^{+\im\frac{2\pi\cdot n \cdot N}{N}} = 1
\end{align}
was erfreulicherweise bestätigt, dass die $NxN$ Einheitsmatrix $\bm{I}$ immer
$N$ Eigenwerte mit Wert 1 hat.
Trivial, aber der Vollständigkeit halber, notieren wir (Matrix hoch 0 ist Einheitsmatrix)
\begin{align}
\bm{P}^N \bm{x}_n = \bm{P}^0 \bm{x}_n = \bm{I} \bm{x}_n = \bm{x}_n
\end{align}

\textbf{Zyklische Matrix als Linearkombination zyklischer Matrizen}
Die lineare Superposition von Vektoren und Matrizen ist ja das Kerngeschäft
der linearen Algebra.
Daher ist ein weiterer logischer Schritt aus den $N$
Permutationsmatrizen eine Linearkombination der Form---hier für $N=4$---
\begin{align}
&\bm{C} = c_1 \, \bm{P}^1 + c_2 \, \bm{P}^2 + c_3 \, \bm{P}^3 + c_0 \, \bm{P}^4\text{ bzw. umsortiert}\nonumber\\
&\bm{C} = c_0 \, \bm{I} + c_1 \, \bm{P}^1 + c_2 \, \bm{P}^2 + c_3 \, \bm{P}^3
\end{align}
zu bauen.
Die Koeffizienten $c_{(\cdot)}$ dürfen komplexwertig sein. Die resultierende
Matrix $\bm{C}$ ist zyklisch, so wie auch die Permutationsmatrizen.
Bleiben wir bei unserem einfach zu überschauendem Beispiel mit $N=4$, dann hat
$\bm{C}$ die Struktur
%
\begin{align}
\bm{C}=
\begin{bmatrix}
c_0& c_1& c_2& c_3\\
c_3& c_0& c_1& c_2\\
c_2& c_3& c_0& c_1\\
c_1& c_2& c_3& c_0
\end{bmatrix}.
\end{align}
Für zyklische Matrizen müssen wir nur die $N$ Einträge der ersten Zeile oder der ersten Spalte kennen,
den Rest können wir gemäß zyklischer Diagonalisierung zusammenbauen.
Die Koeffizienten $c_0,c_1, c_2, c_3,\dots, c_{N-1}$
haben eine besondere Bedeutung.

Da $\bm{C}$ die gleichen Eigenvektoren $\bm{x}_n$ aufweist, wie $\bm{P}$
(und $\bm{P}^2$, $\bm{P}^3$, $\bm{P}^4$), eben weil es eine Linearkombination
ist, können wir das Eigenwertproblem schnell formulieren
\begin{align}
\bm{C} \bm{x}_n = \zeta_n \bm{x}_n.
\end{align}
Um Verwechslung auszuschließen, bezeichnen wir die $N$ möglichen
Eigenwerte von $\bm{C}$ mit $\zeta_n$ für $0\leq n \leq N-1$.
%
Für unser Beispiel $N=4$, also $\zeta_0, \zeta_1, \zeta_2, \zeta_3$.
%
Diese Eigenwerte erhalten wir durch Summation der $c_n$ gewichteten Eigenwerte
der Matrizen $\bm{P}^1, \bm{P}^2, \bm{P}^3, \bm{P}^4$.
Diese Summationsregel gilt für Matrizen mit gleichen! Eigenvektoren, was hier
ja intentional vorliegt. Allgemein für den $n$-ten Eigenwert von $\bm{C}$
gilt somit
\begin{align}
&\zeta_n = c_0 + c_1 \lambda_n + c_2 \lambda_n^2 + c_3 \lambda_n^3 + \dots + c_{N-1} \lambda_n^{N-1}\\
&\zeta_n = c_0 + c_1 W_N^n + c_2 (W_N^n)^2 + c_3 (W_N^n)^3 + \dots + c_{N-1} (W_N^n)^{N-1}\\
&\zeta_n = c_0 + c_1 W_N^n + c_2 W_N^{2n} + c_3 W_N^{3n} + \dots + c_{N-1} W_N^{(N-1) n}
\end{align}

Schreiben wir das mal aus für $0\leq n \leq N-1$:
\begin{align}
&\zeta_0 = c_0 + c_1 W_N^0 + c_2 W_N^{2\cdot0} + c_3 W_N^{3\cdot0} + \dots + c_{N-1} W_N^{(N-1) \cdot 0}\nonumber\\
&\zeta_1 = c_0 + c_1 W_N^1 + c_2 W_N^{2\cdot1} + c_3 W_N^{3\cdot1} + \dots + c_{N-1} W_N^{(N-1) \cdot1}\nonumber\\
&\zeta_2 = c_0 + c_1 W_N^2 + c_2 W_N^{2\cdot2} + c_3 W_N^{3\cdot2} + \dots + c_{N-1} W_N^{(N-1) \cdot2}\nonumber\\
&\zeta_3 = c_0 + c_1 W_N^3 + c_2 W_N^{2\cdot3} + c_3 W_N^{3\cdot3} + \dots + c_{N-1} W_N^{(N-1) \cdot3}\nonumber\\
&\vdots\nonumber\\
&\zeta_{N-1} = c_0 + c_1 W_N^{N-1} + c_2 W_N^{2\cdot(N-1)} + c_3 W_N^{3\cdot(N-1)} + \dots + c_{N-1} W_N^{(N-1) \cdot(N-1)}
\end{align}
Das ist ein Gleichungssystem und lässt sich bestens in Matrix-Notation schreiben.
Definieren wir die Spaltenvektoren $\bm{\zeta} = [\zeta_0, \zeta_1, \zeta_2, \zeta_3, \dots, \zeta_{N-1}]^\mathrm{T}$
und $\bm{c} = [c_0, c_1, c_2, c_3, \dots, c_{N-1}]^\mathrm{T}$ und schauen das obige
Gleichungssystem im Vergleich zu \eq{eq:DFT_FMatrix_WN} (hier nochmal gegeben)
\begin{align*}
\bm{F}_N =
\begin{bmatrix}
1 & 1 & 1 & 1 & \dots & 1\\[1em]
1 & W_N^1 & W_N^2 & W_N^3 & \dots & W_N^{(N-1)}\\[1em]
1 & W_N^2 & W_N^4 & W_N^6 & \dots & W_N^{2(N-1)}\\[1em]
1 & W_N^3 & W_N^6 & W_N^9 & \dots & W_N^{3(N-1)}\\[1em]
\vdots & \vdots & \vdots &\vdots &\ddots & \vdots\\[1em]
1 & W_N^{(N-1)} & W_N^{2(N-1)} & W_N^{3(N-1)} & \dots & W_N^{(N-1)(N-1)}
\end{bmatrix}
\end{align*}
genau an, dann finden wir
\begin{align}
\label{eq:Zeta_F_c}
\bm{\zeta} = \bm{F} \bm{c},
\end{align}
also (in unserer Konvention) die inverse DFT ohne die $\frac{1}{N}$-Normierung.
%
Die $N$ Eigenwerte in $\bm{\zeta}$ der zyklischen Matrix $\bm{C}$ errechnen sich
aus $\bm{F} \bm{c}$, wobei der Spaltenvektor $\bm{c} = [c_0, c_1, c_2, ... c_{N-1}]^\mathrm{T}$
die Einträge der ersten Zeile in $\bm{C}$ definiert.
%
Oder anders: wir legen eine zyklische Matrix $\bm{C}$ mit den Koeffizienten
$\bm{c}$ an. Die mit $\bm{c}$ gewichtete Linearkombination
der $N$ orthogonalen Vektoren in der Fouriermatrix $\bm{F}$ ergibt die Eigenwerte
von $\bm{C}$, eingetragen als Koeffizienten in $\bm{\zeta}$ !!

DFT und inverse DFT können wir also als Eigenwertprobleme
zyklischer Matrizen auffassen, wobei
wir immer mit der gleichen orthogonalen, $N$-dimensionalen Basis
für Folgen der Länge $N$ operieren, weil der aufspannbare Vektorraum
offensichtlich sehr elegant ist. Bei der komplexen Fourierreihe hatten wir die Basisfunktionen $\e^{\pm\im\omega_0 \mu t}$ aus ähnlicher Motivation gewählt.
Die Fouriermatrix ist im Wesen die Analogie für (zeit-/frequenzdiskrete)
Folgen, die periodisch aufgefasst werden.

Für unser Beispiel $N=4$ könnten wir das Gleichungssystem aufstellen
\begin{align}
\begin{bmatrix}
\zeta_0 \\ \zeta_1 \\ \zeta_2 \\ \zeta_3
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & \im & -1 & -\im \\
1 & -1 & 1 & -1 \\
1 & -\im & -1 & \im
\end{bmatrix}
\,
\begin{bmatrix}
c_0 \\ c_1 \\ c_2 \\ c_3
\end{bmatrix},
\end{align}
bzw. als schön geschriebene Linearkombination
(in unserer Konvention ist das die inverse DFT ohne Berücksichtigung von $\frac{1}{N}$!)
\begin{align}
\begin{bmatrix}
\zeta_0 \\ \zeta_1 \\ \zeta_2 \\ \zeta_3
\end{bmatrix}
=
c_0
\begin{bmatrix}
1\\1\\1\\1
\end{bmatrix}+
c_1
\begin{bmatrix}
1\\\im\\-1\\-\im
\end{bmatrix}+
c_2
\begin{bmatrix}
1\\-1\\1\\-1
\end{bmatrix}+
c_3
\begin{bmatrix}
1\\-\im\\-1\\\im
\end{bmatrix}
\end{align}

Wenn wir nun $\bm{c}$ als DFT-Spektral-Koeffizienten (Frequenzbereich) und
den Vektor $\bm{\zeta}$ als Folge von Samples (Zeitbereich) auffassen,
dann können wir uns das Wesen an drei anschaulichen Beispielen klar machen.

I. Für $\bm{c} = [1,0,0,0]^\mathrm{T}$ folgt $\bm{\zeta} = [1,1,1,1]^\mathrm{T}$,
das ist die Synthese des Gleichanteils. Das Spektrum enthält nur einen Dirac Impulse
in $c_0$ und repräsentiert die DFT-Frequenz $\Omega = 0 \cdot \frac{2\pi}{4}$.

II. Für $\bm{c} = [0,0,1,0]^\mathrm{T}$ folgt $\bm{\zeta} = [1,-1,1,-1]^\mathrm{T}$,
das ist die Synthese der schnellst möglich schwingenden Folge (i.e. halbe Abtastfrequenz).
Das Spektrum enthält nur einen Dirac Impuls in $c_2$
und repräsentiert die DFT-Frequenz $\Omega = 2 \cdot \frac{2\pi}{4} = \pi$.

III. Für $\bm{c} = [\nicefrac{1}{4},\nicefrac{1}{4},\nicefrac{1}{4},\nicefrac{1}{4}]^\mathrm{T}$
folgt $\bm{\zeta} = [1,0,0,0]^\mathrm{T}$,
das ist die Synthese eines Dirac Impulses bei $k=0$.
Das Spektrum $\bm{c}$ enthält alle Frequenzen gleich gewichtet.

\textbf{Zyklische Faltung} {\tiny 02A5968B56}
Für zwei zyklische Matrizen können wir per Multiplikation eine
neue zyklische Matrix erzeugen, z.B.
\begin{align}
\bm{X} =
\begin{bmatrix}
-1 &  4 &  2\\
 2 & -1 &  4\\
 4 &  2 & -1
\end{bmatrix}
\qquad
\bm{H} =
\begin{bmatrix}
3 & 5 & 1\\
1 & 3 & 5\\
5 & 1 & 3
\end{bmatrix}
\qquad
\bm{Y} = \bm{X}\bm{H} =
\begin{bmatrix}
11  &   9  &  25\\
25  &  11  &  9\\
 9  &  25  &  11
\end{bmatrix}
\end{align}
In den jeweils ersten Spalten stehen die Signalvektoren für $x[k]$, $h[k]$ und $y[k]$
so wie wir es in SigSys typisch benutzen, als Spaltenvektoren $\bm{x}$, $\bm{h}$, $\bm{y}$.
%
Nachdem wir wissen, dass die Ergebnismatrix zyklisch ist, brauchen wir nur
eine Spalte oder eine Zeile ausrechnen und können des Rest auffüllen.
%
Eine Linearkombination der Spalten von $\bm{X}$ mit den Gewichten in der
ersten Spalte von $\bm{H}$ ergibt die erste Ergebnisspalte von $\bm{Y}=\bm{X} \bm{H}$
\begin{align}
3\cdot
\begin{bmatrix}
-1\\
 2\\
 4
\end{bmatrix}
+
1\cdot
\begin{bmatrix}
4\\
-1\\
2
\end{bmatrix}
+5\cdot
\begin{bmatrix}
2\\
4\\
-1
\end{bmatrix}
=
\begin{bmatrix}
11\\
25\\
 9
\end{bmatrix}
\end{align}
Eine Linearkombination der Zeilen von $\bm{H}$ mit den Gewichten in der ersten Zeile
von $\bm{X}$ ergibt die erste Ergebniszeile von $\bm{Y}=\bm{X} \bm{H}$
\begin{align}
\label{eq:C8864C8D9F_LinComb_Row}
-1 \cdot [3 \quad 5 \quad 1] + 4 \cdot [1 \quad 3 \quad 5] + 2 \cdot [5 \quad 1 \quad 3] =
[11 \quad 9 \quad 25]
\end{align}
Damit können wir die zyklische Struktur auffüllen und müssten nicht
weiterrechnen.
%
Wir können die zyklische Faltung
\begin{align}
\begin{bmatrix}
-1\\2\\4
\end{bmatrix}
\circledast_3
\begin{bmatrix}
3\\1\\5
\end{bmatrix}
=
\begin{bmatrix}
11\\25\\9
\end{bmatrix}
= \bm{y}
\end{align}
ausrechnen, und finden die Äquivalenz mit den jeweils ersten Spalten der Matrizen
$\bm{X}$, $\bm{H}$ und $\bm{Y} = \bm{X} \bm{H}$.
Die Matrixoperation $\bm{X} \bm{H}$
realisiert also die zyklische Faltung,
eben weil wir zyklische Matrizen benutzen. Rufen wir uns noch Erinnerung,
dass bei zyklischen Matrizen $\bm{X} \bm{H} = \bm{H} \bm{X}$ gilt, also
die Kommutativität der (zyklischen) Faltung konsistent abbildet.

Einer der wichtigsten Zusammenhänge der SigSys findet sich natürlich auch
in der Algebra wieder. Es ist der Link zwischen Zeit- und Bildbereich, oder
in Algebra-Sprech: zwischen Vektor und Eigenwert der zyklischen Matrix, die von diesem
Vektor erzeugt wird. Statt \eqref{eq:Zeta_F_c} benutzen wir hier nun die Eigenwerte
die mit $\bm{F}^\mathrm{H}$ verknüpft sind, es funktioniert ganz äquivalent.
%
Die DFT des zyklischen Faltungsergebnisses (linke Seite)
ist gleich der elementweisen Multiplikation ($\odot$) der DFT Koeffizienten (rechte Seite):
\begin{align}
\bm{F}^\mathrm{H}(\bm{x} \circledast_N \bm{h}) = (\bm{F}^\mathrm{H}\bm{x})  \odot (\bm{F}^\mathrm{H}\bm{h}).
\end{align}
Linksseitige Multiplikation
\begin{align}
\frac{1}{N} \bm{F}\bm{F}^\mathrm{H}(\bm{x} \circledast_N \bm{h}) =
\frac{1}{N} \bm{F}\left( (\bm{F}^\mathrm{H}\bm{x})  \odot (\bm{F}^\mathrm{H}\bm{h}) \right)
\end{align}
erzeugt wegen $\frac{1}{N} \bm{F}\bm{F}^\mathrm{H} = \bm{I}$ die Zusammenhänge
\begin{align}
&\bm{y} = \bm{x} \circledast_N \bm{h} =
\frac{1}{N} \bm{F} \left( \bm{F}^\mathrm{H}\bm{x}  \odot \bm{F}^\mathrm{H}\bm{h} \right)\\
&\bm{y} = \bm{x} \circledast_N \bm{h} =
\frac{1}{N} \bm{F}\left( \mathrm{eigvals}(\bm{X})  \odot \mathrm{eigvals}(\bm{H}) \right)\\
&\bm{y} = \bm{x} \circledast_N \bm{h} = \mathrm{IDFT}_N\{\quad  \mathrm{DFT}_N\{\bm{x}\} \odot  \mathrm{DFT}_N\{\bm{h}\} \quad \}
\end{align}
Dies ist der wichtige Zusammenhang zwischen zyklischer Faltung von zyklischen Folgen und
elementweiser Multiplikation der Eigenwerte der jeweiligen zyklischen Matrizen
(Achtung: die Eigenwerte von $\bm{X}$ und $\bm{H}$ müssen natürlich zum jeweilig gleichen
Eigenvektor gehören, das geht in der Formelschreibweise eigvals() ein wenig unter).
%

Der Gag an diesem Umweg:
Für sehr große $N$ ist die rechte Seite deutlich recheneffizienter, weil
die benötigten Matrixmultiplikationen sehr stark optimiert werden können, die benötigte
Faltungssumme auf der linken Seite hingegen nicht.
%
Diese Optimierungen, also effiziente Algorithmen zur Matrixmultiplikation
sind seit den 1960er Jahren unter dem Sammelbegriff Fast Fourier Transform
(FFT) erforscht und realisiert.
Es gibt also nicht die eine FFT, sondern für spezielle $N$, für
spezielle Rechner, für bestimmten Speicheraufwand usw.
optimierte Algorithmen für schnellstmögliche Matrixmultiplikationen mit
$\bm{F}$ bzw. $\bm{F}^\mathrm{*}$. Mit dem Design von FFT Algorithmen sind ganze
Ingenieurslebensläufe bestritten worden. Die \texttt{fft()} in Matlab und
scipy/numpy für Python basiert auf hoch-optimierten FFT-Bibliotheken für
typische CPU-Anwendungen.

Der Erfolg von Data Science heute begründet sich neben leistungsfähiger Hardware
vor allem durch die sehr recheneffiziente Umsetzung von DFT und SVD.
